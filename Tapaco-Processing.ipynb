{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbff25ba-5578-4408-90dc-0a1929879af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T12:38:30.418689Z",
     "iopub.status.busy": "2023-05-21T12:38:30.418362Z",
     "iopub.status.idle": "2023-05-21T12:38:30.423896Z",
     "shell.execute_reply": "2023-05-21T12:38:30.423440Z",
     "shell.execute_reply.started": "2023-05-21T12:38:30.418638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['https_proxy'] = \"http://proxy.hcm.fpt.vn:80/\"\n",
    "os.environ['http_proxy'] = \"http://proxy.hcm.fpt.vn:80/\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0daf0ea7-0769-486c-8c65-c76b49b0eef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T12:38:31.202627Z",
     "iopub.status.busy": "2023-05-21T12:38:31.202370Z",
     "iopub.status.idle": "2023-05-21T12:38:32.059499Z",
     "shell.execute_reply": "2023-05-21T12:38:32.058958Z",
     "shell.execute_reply.started": "2023-05-21T12:38:31.202610Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: unexpected EOF while looking for matching `\"'\n",
      "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
     ]
    }
   ],
   "source": [
    "!git config --global http.proxy \"http://proxy.hcm.fpt.vn:80\"\n",
    "!git config --global http.https://git.bigdata.local.proxy \"\"\"\n",
    "!export http_proxy=proxy.hcm.fpt.vn\n",
    "!export https_proxy=proxy.hcm.fpt.vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9297263c-61a0-49d3-a769-9108ef361da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T12:38:33.054302Z",
     "iopub.status.busy": "2023-05-21T12:38:33.053443Z",
     "iopub.status.idle": "2023-05-21T12:38:37.463235Z",
     "shell.execute_reply": "2023-05-21T12:38:37.462676Z",
     "shell.execute_reply.started": "2023-05-21T12:38:33.054261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3242842-44ae-4d54-8977-f8f6fb66857a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T12:38:37.464455Z",
     "iopub.status.busy": "2023-05-21T12:38:37.464150Z",
     "iopub.status.idle": "2023-05-21T12:38:51.339793Z",
     "shell.execute_reply": "2023-05-21T12:38:51.339203Z",
     "shell.execute_reply.started": "2023-05-21T12:38:37.464438Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tapaco (/home/sinhns2/.cache/huggingface/datasets/tapaco/en/1.0.0/71d200534b520a174927a8f0479c06220a0a6fb5201a84ebfce19006c6354698)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d01066ae4d540319a3abb29319a109e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the 'tapaco' dataset in English\n",
    "dataset = load_dataset(\"tapaco\", \"en\")\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa86c7a3-6353-4440-85ab-b214f0730690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T12:39:07.093375Z",
     "iopub.status.busy": "2023-05-21T12:39:07.092894Z",
     "iopub.status.idle": "2023-05-21T12:39:07.442141Z",
     "shell.execute_reply": "2023-05-21T12:39:07.441657Z",
     "shell.execute_reply.started": "2023-05-21T12:39:07.093345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paraphrase_set_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>paraphrase</th>\n",
       "      <th>lists</th>\n",
       "      <th>tags</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>416554</td>\n",
       "      <td>I ate the cheese.</td>\n",
       "      <td>[907, 4000, 6677, 7361, 7415]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2481696</td>\n",
       "      <td>I eat cheese.</td>\n",
       "      <td>[907]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2721028</td>\n",
       "      <td>I'm eating a yogurt.</td>\n",
       "      <td>[992, 3800]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3010891</td>\n",
       "      <td>I'm eating cheese.</td>\n",
       "      <td>[6905]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4129977</td>\n",
       "      <td>I'm having some cheese.</td>\n",
       "      <td>[6905]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158048</th>\n",
       "      <td>843414</td>\n",
       "      <td>941276</td>\n",
       "      <td>All of you are welcome to attend my party alon...</td>\n",
       "      <td>[649]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158049</th>\n",
       "      <td>844191</td>\n",
       "      <td>962048</td>\n",
       "      <td>I wouldn't like being a judge.</td>\n",
       "      <td>[907, 4000, 7414]</td>\n",
       "      <td>[8 syllables]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158050</th>\n",
       "      <td>844191</td>\n",
       "      <td>962049</td>\n",
       "      <td>I wouldn't want to be a judge.</td>\n",
       "      <td>[907, 4000, 7414]</td>\n",
       "      <td>[8 syllables]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158051</th>\n",
       "      <td>844308</td>\n",
       "      <td>988514</td>\n",
       "      <td>To attend is important!</td>\n",
       "      <td>[649]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158052</th>\n",
       "      <td>844308</td>\n",
       "      <td>988515</td>\n",
       "      <td>Attending is important!</td>\n",
       "      <td>[649]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158053 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       paraphrase_set_id sentence_id  \\\n",
       "0                      1      416554   \n",
       "1                      1     2481696   \n",
       "2                      1     2721028   \n",
       "3                      1     3010891   \n",
       "4                      1     4129977   \n",
       "...                  ...         ...   \n",
       "158048            843414      941276   \n",
       "158049            844191      962048   \n",
       "158050            844191      962049   \n",
       "158051            844308      988514   \n",
       "158052            844308      988515   \n",
       "\n",
       "                                               paraphrase  \\\n",
       "0                                       I ate the cheese.   \n",
       "1                                           I eat cheese.   \n",
       "2                                    I'm eating a yogurt.   \n",
       "3                                      I'm eating cheese.   \n",
       "4                                 I'm having some cheese.   \n",
       "...                                                   ...   \n",
       "158048  All of you are welcome to attend my party alon...   \n",
       "158049                     I wouldn't like being a judge.   \n",
       "158050                     I wouldn't want to be a judge.   \n",
       "158051                            To attend is important!   \n",
       "158052                            Attending is important!   \n",
       "\n",
       "                                lists           tags language  \n",
       "0       [907, 4000, 6677, 7361, 7415]             []       en  \n",
       "1                               [907]             []       en  \n",
       "2                         [992, 3800]             []       en  \n",
       "3                              [6905]             []       en  \n",
       "4                              [6905]             []       en  \n",
       "...                               ...            ...      ...  \n",
       "158048                          [649]             []       en  \n",
       "158049              [907, 4000, 7414]  [8 syllables]       en  \n",
       "158050              [907, 4000, 7414]  [8 syllables]       en  \n",
       "158051                          [649]             []       en  \n",
       "158052                          [649]             []       en  \n",
       "\n",
       "[158053 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c312b602-0b45-4c44-a15b-6f38723be9c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T08:36:54.413524Z",
     "iopub.status.busy": "2023-05-14T08:36:54.412800Z",
     "iopub.status.idle": "2023-05-14T08:36:54.421516Z",
     "shell.execute_reply": "2023-05-14T08:36:54.420174Z",
     "shell.execute_reply.started": "2023-05-14T08:36:54.413466Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_tapaco_paraphrase_dataset(dataset, out_file):\n",
    "    dataset_df = dataset[[\"paraphrase\", \"paraphrase_set_id\"]]\n",
    "    non_single_labels = (\n",
    "        dataset_df[\"paraphrase_set_id\"]\n",
    "        .value_counts()[dataset_df[\"paraphrase_set_id\"].value_counts() > 1]\n",
    "        .index.tolist()\n",
    "    )\n",
    "    tapaco_df_sorted = dataset_df.loc[\n",
    "        dataset_df[\"paraphrase_set_id\"].isin(non_single_labels)\n",
    "    ]\n",
    "    tapaco_paraphrases_dataset = []\n",
    "\n",
    "    for paraphrase_set_id in tqdm(tapaco_df_sorted[\"paraphrase_set_id\"].unique()):\n",
    "        id_wise_paraphrases = tapaco_df_sorted[\n",
    "            tapaco_df_sorted[\"paraphrase_set_id\"] == paraphrase_set_id\n",
    "        ]\n",
    "        len_id_wise_paraphrases = (\n",
    "            id_wise_paraphrases.shape[0]\n",
    "            if id_wise_paraphrases.shape[0] % 2 == 0\n",
    "            else id_wise_paraphrases.shape[0] - 1\n",
    "        )\n",
    "        for ix in range(0, len_id_wise_paraphrases, 2):\n",
    "            current_phrase = id_wise_paraphrases.iloc[ix][0]\n",
    "            for count_ix in range(ix + 1, ix + 2):\n",
    "                next_phrase = id_wise_paraphrases.iloc[ix + 1][0]\n",
    "                tapaco_paraphrases_dataset.append([current_phrase, next_phrase])\n",
    "    tapaco_paraphrases_dataset_df = pd.DataFrame(\n",
    "        tapaco_paraphrases_dataset, columns=[\"Text\", \"Paraphrase\"]\n",
    "    )\n",
    "    tapaco_paraphrases_dataset_df.to_csv(out_file, sep=\"\\t\", index=None)\n",
    "    return tapaco_paraphrases_dataset_df\n",
    "\n",
    "tapaco_paraphrases_dataset_df = generate_tapaco_paraphrase_dataset(df,\"tapaco_paraphrases_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63deb4c-e932-49f9-8f73-cf26f62260db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T08:31:06.278531Z",
     "iopub.status.busy": "2023-05-14T08:31:06.278131Z",
     "iopub.status.idle": "2023-05-14T08:31:06.378137Z",
     "shell.execute_reply": "2023-05-14T08:31:06.377772Z",
     "shell.execute_reply.started": "2023-05-14T08:31:06.278509Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Paraphrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I ate the cheese.</td>\n",
       "      <td>I eat cheese.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm eating a yogurt.</td>\n",
       "      <td>I'm eating cheese.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm having some cheese.</td>\n",
       "      <td>I eat some cheese.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's Monday.</td>\n",
       "      <td>It is Monday today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's Monday today.</td>\n",
       "      <td>Today is Monday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73513</th>\n",
       "      <td>What's your favorite horror movie?</td>\n",
       "      <td>What's your favorite scary movie?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73514</th>\n",
       "      <td>Globalise or die.</td>\n",
       "      <td>Globalize or die.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73515</th>\n",
       "      <td>All of you are welcome to bring your spouses t...</td>\n",
       "      <td>All of you are welcome to attend my party alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73516</th>\n",
       "      <td>I wouldn't like being a judge.</td>\n",
       "      <td>I wouldn't want to be a judge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73517</th>\n",
       "      <td>To attend is important!</td>\n",
       "      <td>Attending is important!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73518 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "0                                      I ate the cheese.   \n",
       "1                                   I'm eating a yogurt.   \n",
       "2                                I'm having some cheese.   \n",
       "3                                           It's Monday.   \n",
       "4                                     It's Monday today.   \n",
       "...                                                  ...   \n",
       "73513                 What's your favorite horror movie?   \n",
       "73514                                  Globalise or die.   \n",
       "73515  All of you are welcome to bring your spouses t...   \n",
       "73516                     I wouldn't like being a judge.   \n",
       "73517                            To attend is important!   \n",
       "\n",
       "                                              Paraphrase  \n",
       "0                                          I eat cheese.  \n",
       "1                                     I'm eating cheese.  \n",
       "2                                     I eat some cheese.  \n",
       "3                                    It is Monday today.  \n",
       "4                                       Today is Monday.  \n",
       "...                                                  ...  \n",
       "73513                  What's your favorite scary movie?  \n",
       "73514                                  Globalize or die.  \n",
       "73515  All of you are welcome to attend my party alon...  \n",
       "73516                     I wouldn't want to be a judge.  \n",
       "73517                            Attending is important!  \n",
       "\n",
       "[73518 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tapaco_paraphrases_dataset_df = pd.read_csv(\"tapaco_paraphrases_dataset.csv\", sep=\"\\t\")\n",
    "tapaco_paraphrases_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face4d4b-8a98-467c-b4b7-60eac813e086",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc67a05c-d426-4674-b38a-8a2767758a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T06:29:37.059925Z",
     "iopub.status.busy": "2023-05-13T06:29:37.059622Z",
     "iopub.status.idle": "2023-05-13T06:29:38.327710Z",
     "shell.execute_reply": "2023-05-13T06:29:38.327225Z",
     "shell.execute_reply.started": "2023-05-13T06:29:37.059905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf==3.20.1 in /opt/conda/lib/python3.8/site-packages (3.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb082ff-e0c7-4781-a87f-fc89c5f2fe96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T08:37:01.857177Z",
     "iopub.status.busy": "2023-05-14T08:37:01.856476Z",
     "iopub.status.idle": "2023-05-14T08:37:47.378210Z",
     "shell.execute_reply": "2023-05-14T08:37:47.376630Z",
     "shell.execute_reply.started": "2023-05-14T08:37:01.857120Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 15:37:02.048433: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-14 15:37:12.194018: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-14 15:37:12.194851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-14 15:37:12.194887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4166b293-dc20-4122-922b-56b10b429265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T02:42:34.492504Z",
     "iopub.status.busy": "2023-05-20T02:42:34.491663Z",
     "iopub.status.idle": "2023-05-20T02:42:34.995707Z",
     "shell.execute_reply": "2023-05-20T02:42:34.994282Z",
     "shell.execute_reply.started": "2023-05-20T02:42:34.492441Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tapaco_paraphrases_dataset_df = pd.read_csv(\"tapaco_paraphrases_dataset.csv\", sep=\"\\t\")\n",
    "tapaco_paraphrases_dataset_df.columns = [\"input_text\",\"target_text\"]\n",
    "tapaco_paraphrases_dataset_df[\"prefix\"] = \"paraphrase\"\n",
    "train_data,val_data = train_test_split(tapaco_paraphrases_dataset_df,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6567470f-afcc-4983-979b-191668c5d70e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T02:42:38.019075Z",
     "iopub.status.busy": "2023-05-20T02:42:38.018250Z",
     "iopub.status.idle": "2023-05-20T02:42:39.205791Z",
     "shell.execute_reply": "2023-05-20T02:42:39.204298Z",
     "shell.execute_reply.started": "2023-05-20T02:42:38.019011Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40775</th>\n",
       "      <td>We haven't seen him.</td>\n",
       "      <td>We haven't seen her.</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8494</th>\n",
       "      <td>Anyone can do that.</td>\n",
       "      <td>Anyone can do it.</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49418</th>\n",
       "      <td>We will probably arrive at Tokyo station at noon.</td>\n",
       "      <td>It's possible we'll arrive at the Tokyo statio...</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45189</th>\n",
       "      <td>Why do I always need to struggle with such pro...</td>\n",
       "      <td>Why do I always have to put up with these things?</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10993</th>\n",
       "      <td>This is the first time for me to read the Bible.</td>\n",
       "      <td>It’s the first time that I read the Bible.</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45101</th>\n",
       "      <td>I've seen a couple of Kurosawa's films.</td>\n",
       "      <td>I have seen at least one Kurosawa film.</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57843</th>\n",
       "      <td>Although you rushed, you're not ready.</td>\n",
       "      <td>Even though you're quick, you're still not ready.</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40235</th>\n",
       "      <td>You guys are Swedish.</td>\n",
       "      <td>You all are Swedish.</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>I'm from out of town.</td>\n",
       "      <td>I'm new around here.</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15688</th>\n",
       "      <td>We'll miss him a lot.</td>\n",
       "      <td>We'll miss her a lot.</td>\n",
       "      <td>paraphrase</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_text  \\\n",
       "40775                               We haven't seen him.   \n",
       "8494                                 Anyone can do that.   \n",
       "49418  We will probably arrive at Tokyo station at noon.   \n",
       "45189  Why do I always need to struggle with such pro...   \n",
       "10993   This is the first time for me to read the Bible.   \n",
       "...                                                  ...   \n",
       "45101            I've seen a couple of Kurosawa's films.   \n",
       "57843             Although you rushed, you're not ready.   \n",
       "40235                              You guys are Swedish.   \n",
       "2120                               I'm from out of town.   \n",
       "15688                              We'll miss him a lot.   \n",
       "\n",
       "                                             target_text      prefix  \n",
       "40775                               We haven't seen her.  paraphrase  \n",
       "8494                                   Anyone can do it.  paraphrase  \n",
       "49418  It's possible we'll arrive at the Tokyo statio...  paraphrase  \n",
       "45189  Why do I always have to put up with these things?  paraphrase  \n",
       "10993         It’s the first time that I read the Bible.  paraphrase  \n",
       "...                                                  ...         ...  \n",
       "45101            I have seen at least one Kurosawa film.  paraphrase  \n",
       "57843  Even though you're quick, you're still not ready.  paraphrase  \n",
       "40235                               You all are Swedish.  paraphrase  \n",
       "2120                                I'm new around here.  paraphrase  \n",
       "15688                              We'll miss her a lot.  paraphrase  \n",
       "\n",
       "[7352 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53913992-fe57-482d-b1c8-b551d3f61eb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T07:15:49.959253Z",
     "iopub.status.busy": "2023-05-13T07:15:49.958951Z",
     "iopub.status.idle": "2023-05-13T07:16:00.166602Z",
     "shell.execute_reply": "2023-05-13T07:16:00.166156Z",
     "shell.execute_reply.started": "2023-05-13T07:15:49.959234Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d152c86ba2494283f36e1fd17c6145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define data trainer\n",
    "def tokenize_data(examples):\n",
    "    inputs = [f\"{prefix}: {input_text}\" for prefix, input_text in zip(examples['prefix'], examples['input_text'])]\n",
    "    targets = list(examples['target_text'])\n",
    "    input_encodings = tokenizer.batch_encode_plus(inputs, max_length=512, truncation=True, padding=True)\n",
    "    target_encodings = tokenizer.batch_encode_plus(targets, max_length=512, truncation=True, padding=True)\n",
    "    input_ids = torch.tensor(input_encodings['input_ids'])\n",
    "    input_mask = torch.tensor(input_encodings['attention_mask'])\n",
    "    target_ids = torch.tensor(target_encodings['input_ids'])\n",
    "    target_mask = torch.tensor(target_encodings['attention_mask'])\n",
    "    return {'input_ids': input_ids, 'attention_mask': input_mask, 'labels': target_ids}\n",
    "\n",
    "    \n",
    "# Convert DataFrame to Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "\n",
    "# Tokenize the data\n",
    "train_dataset = train_dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dae94a15-30b8-427a-98b4-a716b3cb7f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T07:16:01.878306Z",
     "iopub.status.busy": "2023-05-13T07:16:01.878010Z",
     "iopub.status.idle": "2023-05-13T07:16:03.025745Z",
     "shell.execute_reply": "2023-05-13T07:16:03.025376Z",
     "shell.execute_reply.started": "2023-05-13T07:16:01.878286Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20d1031086349529e852fbf48b6de0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert DataFrame to Dataset\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Tokenize the data\n",
    "val_dataset = val_dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad91dd2d-56cb-45dd-8ec5-df77a828f6f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T07:18:38.762040Z",
     "iopub.status.busy": "2023-05-13T07:18:38.761754Z",
     "iopub.status.idle": "2023-05-13T07:18:38.765964Z",
     "shell.execute_reply": "2023-05-13T07:18:38.765618Z",
     "shell.execute_reply.started": "2023-05-13T07:18:38.762022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def data_collator(data):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"], dtype=torch.long) for item in data]\n",
    "    attention_mask = [torch.tensor(item[\"attention_mask\"], dtype=torch.long) for item in data]\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return {\n",
    "        \"input_ids\": padded_input_ids,\n",
    "        \"attention_mask\": padded_attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414da65d-bfe5-4b98-994b-9184cc1b9b29",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-14T08:38:00.294347Z",
     "iopub.status.busy": "2023-05-14T08:38:00.293012Z",
     "iopub.status.idle": "2023-05-14T08:38:06.083012Z",
     "shell.execute_reply": "2023-05-14T08:38:06.081346Z",
     "shell.execute_reply.started": "2023-05-14T08:38:00.294283Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[1;32m     16\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     17\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# directory to save checkpoints and logs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# total number of training epochs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     greater_is_better\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# whether the metric for best model should be maximized or minimized\u001b[39;00m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m---> 36\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataset\u001b[49m,\n\u001b[1;32m     37\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m     38\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     42\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load and preprocess data\n",
    "tapaco_paraphrases_dataset_df = pd.read_csv(\"tapaco_paraphrases_dataset.csv\", sep=\"\\t\")\n",
    "tapaco_paraphrases_dataset_df.columns = [\"input_text\", \"target_text\"]\n",
    "tapaco_paraphrases_dataset_df[\"prefix\"] = \"paraphrase\"\n",
    "train_data, val_data = train_test_split(tapaco_paraphrases_dataset_df, test_size=0.1)\n",
    "\n",
    "# Load T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # directory to save checkpoints and logs\n",
    "    num_train_epochs=3,  # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per GPU\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # weight decay coefficient for regularization\n",
    "    logging_dir=\"./logs\",  # directory to save training logs\n",
    "    logging_steps=500,  # number of steps to save training logs\n",
    "    evaluation_strategy=\"steps\",  # evaluation strategy after steps or epochs\n",
    "    eval_steps=500,  # evaluation steps\n",
    "    save_total_limit=3,  # limit to number of checkpoints saved\n",
    "    load_best_model_at_end=True,  # load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # metric to monitor for saving the best model\n",
    "    greater_is_better=False,  # whether the metric for best model should be maximized or minimized\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4354cfd0-58fc-4ad7-8427-d47930361de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T08:56:25.286374Z",
     "iopub.status.busy": "2023-05-14T08:56:25.285973Z",
     "iopub.status.idle": "2023-05-14T08:56:54.860694Z",
     "shell.execute_reply": "2023-05-14T08:56:54.860213Z",
     "shell.execute_reply.started": "2023-05-14T08:56:25.286353Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 15:56:25.365238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-14 15:56:25.483789: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-14 15:56:28.056552: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-14 15:56:28.056630: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-14 15:56:28.056637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ca2f5d607d4330beabde9b397d1f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64b8bcf91414a7e9e0d49dad7fe9ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the tokenizer and the pre-trained T5 model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the Tapaco paraphrase dataset\n",
    "tapaco_paraphrases_dataset_df = pd.read_csv(\"tapaco_paraphrases_dataset.csv\", sep=\"\\t\")\n",
    "tapaco_paraphrases_dataset_df.columns = [\"input_text\",\"target_text\"]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(tapaco_paraphrases_dataset_df, test_size=0.1)\n",
    "\n",
    "# Define the training and validation datasets using the Hugging Face Dataset class\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Define the function to preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"input_text\"]\n",
    "    targets = examples[\"target_text\"]\n",
    "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        model_targets = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    model_inputs[\"labels\"] = model_targets[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Preprocess the training and validation datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552d4780-0100-49e1-a8e3-61575c64feb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T08:46:40.508273Z",
     "iopub.status.busy": "2023-05-14T08:46:40.507987Z",
     "iopub.status.idle": "2023-05-14T08:46:50.842395Z",
     "shell.execute_reply": "2023-05-14T08:46:50.841994Z",
     "shell.execute_reply.started": "2023-05-14T08:46:40.508256Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init()\n",
    "# ... calculate metrics, generate media\n",
    "wandb.log({\"accuracy\": 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786bddaa-cba9-4b99-b6c2-5d4aa270257a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-14T08:59:40.552093Z",
     "iopub.status.busy": "2023-05-14T08:59:40.551799Z",
     "iopub.status.idle": "2023-05-14T08:59:40.614854Z",
     "shell.execute_reply": "2023-05-14T08:59:40.614336Z",
     "shell.execute_reply.started": "2023-05-14T08:59:40.552075Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "false INTERNAL ASSERT FAILED at \"../c10/cuda/CUDAGraphsC10Utils.h\":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus22094",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m     10\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./similar_question_t5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Create trainer and start training\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Save fine-tuned model\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/python38/lib/python3.8/site-packages/transformers/trainer.py:315\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/python38/lib/python3.8/site-packages/transformers/trainer_utils.py:93\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     91\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/python38/lib/python3.8/site-packages/torch/random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n",
      "File \u001b[0;32m~/envs/python38/lib/python3.8/site-packages/torch/cuda/random.py:113\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    110\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    111\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 113\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/python38/lib/python3.8/site-packages/torch/cuda/__init__.py:165\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(callable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 165\u001b[0m         \u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m~/envs/python38/lib/python3.8/site-packages/torch/cuda/random.py:111\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    110\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: false INTERNAL ASSERT FAILED at \"../c10/cuda/CUDAGraphsC10Utils.h\":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus22094"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "# Set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./similar_question_t5\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=5,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=50,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=1000,\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "# Create trainer and start training\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_similar_question_generation_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d585c266-f692-44ae-ac39-cfe8defe0456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T08:55:15.485375Z",
     "iopub.status.busy": "2023-05-14T08:55:15.485086Z",
     "iopub.status.idle": "2023-05-14T08:55:15.832761Z",
     "shell.execute_reply": "2023-05-14T08:55:15.832337Z",
     "shell.execute_reply.started": "2023-05-14T08:55:15.485358Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.221\n",
      "Build cuda_11.0_bu.TC445_37.28845127_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3dc9b2-0d1e-4302-a7d9-f88032351614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T11:42:51.218277Z",
     "iopub.status.busy": "2023-05-14T11:42:51.217982Z",
     "iopub.status.idle": "2023-05-14T12:22:16.681213Z",
     "shell.execute_reply": "2023-05-14T12:22:16.680768Z",
     "shell.execute_reply.started": "2023-05-14T11:42:51.218258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_text, target_text, __index_level_0__. If input_text, target_text, __index_level_0__ are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 66166\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4136\n",
      "  Number of trainable parameters = 60506624\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4136' max='4136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4136/4136 39:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Configuration saved in ./results/checkpoint-500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Configuration saved in ./results/checkpoint-1000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Configuration saved in ./results/checkpoint-1500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Configuration saved in ./results/checkpoint-2000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Configuration saved in ./results/checkpoint-2500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n",
      "Configuration saved in ./results/checkpoint-3000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n",
      "Configuration saved in ./results/checkpoint-3500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n",
      "Configuration saved in ./results/checkpoint-4000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./fine_tuned_similar_question_generation_model_t5_small_v2\n",
      "Configuration saved in ./fine_tuned_similar_question_generation_model_t5_small_v2/config.json\n",
      "Configuration saved in ./fine_tuned_similar_question_generation_model_t5_small_v2/generation_config.json\n",
      "Model weights saved in ./fine_tuned_similar_question_generation_model_t5_small_v2/pytorch_model.bin\n",
      "tokenizer config file saved in ./fine_tuned_similar_question_generation_model_t5_small_v2/tokenizer_config.json\n",
      "Special tokens file saved in ./fine_tuned_similar_question_generation_model_t5_small_v2/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_similar_question_generation_model_t5_small_v2/tokenizer_config.json',\n",
       " './fine_tuned_similar_question_generation_model_t5_small_v2/special_tokens_map.json',\n",
       " './fine_tuned_similar_question_generation_model_t5_small_v2/spiece.model',\n",
       " './fine_tuned_similar_question_generation_model_t5_small_v2/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments \n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the tokenizer and the pre-trained T5 model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Load the Tapaco paraphrase dataset\n",
    "tapaco_paraphrases_dataset_df = pd.read_csv(\"tapaco_paraphrases_dataset.csv\", sep=\"\\t\")\n",
    "tapaco_paraphrases_dataset_df.columns = [\"input_text\",\"target_text\"]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(tapaco_paraphrases_dataset_df, test_size=0.1)\n",
    "\n",
    "# Define the training and validation datasets using the Hugging Face Dataset class\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Define the function to preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"input_text\"]\n",
    "    targets = examples[\"target_text\"]\n",
    "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        model_targets = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    model_inputs[\"labels\"] = model_targets[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Preprocess the training and validation datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_total_limit=5,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000\n",
    ")\n",
    "\n",
    "# Define the trainer and train the model\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./fine_tuned_similar_question_generation_model_t5_small_v2\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_similar_question_generation_model_t5_small_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bdbbb-5208-41d3-ba7b-6bf0bc78e2ca",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-14T11:29:31.572325Z",
     "iopub.status.busy": "2023-05-14T11:29:31.572029Z",
     "iopub.status.idle": "2023-05-14T11:29:32.367009Z",
     "shell.execute_reply": "2023-05-14T11:29:32.366494Z",
     "shell.execute_reply.started": "2023-05-14T11:29:31.572308Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./fine_tuned_similar_question_generation_model_t5_small/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"./fine_tuned_similar_question_generation_model_t5_small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ./fine_tuned_similar_question_generation_model_t5_small/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./fine_tuned_similar_question_generation_model_t5_small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./fine_tuned_similar_question_generation_model_t5_small/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for './fine_tuned_similar_question_generation_model_t5_small'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './fine_tuned_similar_question_generation_model_t5_small' is the correct path to a directory containing all relevant files for a T5Tokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_similar_question_generation_model_t5_small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./fine_tuned_similar_question_generation_model_t5_small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/python38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1785\u001b[0m     )\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1788\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1789\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for './fine_tuned_similar_question_generation_model_t5_small'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './fine_tuned_similar_question_generation_model_t5_small' is the correct path to a directory containing all relevant files for a T5Tokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"./fine_tuned_similar_question_generation_model_t5_small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./fine_tuned_similar_question_generation_model_t5_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ce128-102e-45ca-a042-9b0230afd98b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T10:14:56.339081Z",
     "iopub.status.busy": "2023-05-14T10:14:56.338757Z",
     "iopub.status.idle": "2023-05-14T10:14:59.523233Z",
     "shell.execute_reply": "2023-05-14T10:14:59.522805Z",
     "shell.execute_reply.started": "2023-05-14T10:14:56.339064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./fine_tuned_similar_question_generation_model_t5_small\n",
      "Configuration saved in ./fine_tuned_similar_question_generation_model_t5_small/config.json\n",
      "Configuration saved in ./fine_tuned_similar_question_generation_model_t5_small/generation_config.json\n",
      "Model weights saved in ./fine_tuned_similar_question_generation_model_t5_small/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "load_tokenizer = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4e096-6eb6-43f9-80d6-08415e53e439",
   "metadata": {},
   "source": [
    "### Continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b6b9306-8203-44ee-b157-10b5c3235748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:43:57.788001Z",
     "iopub.status.busy": "2023-05-21T05:43:57.787696Z",
     "iopub.status.idle": "2023-05-21T05:44:26.245987Z",
     "shell.execute_reply": "2023-05-21T05:44:26.245315Z",
     "shell.execute_reply.started": "2023-05-21T05:43:57.787980Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# Load the Tapaco paraphrase dataset\n",
    "tapaco_paraphrases_dataset_df = pd.read_csv(\"tapaco_paraphrases_dataset.csv\", sep=\"\\t\")\n",
    "tapaco_paraphrases_dataset_df.columns = [\"input_text\",\"target_text\"]\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(tapaco_paraphrases_dataset_df, test_size=0.1)\n",
    "\n",
    "# Define the training and validation datasets using the Hugging Face Dataset class\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e785b22-83ea-47ce-9323-df44bd0839da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:38:37.746332Z",
     "iopub.status.busy": "2023-05-21T05:38:37.746014Z",
     "iopub.status.idle": "2023-05-21T05:38:58.806149Z",
     "shell.execute_reply": "2023-05-21T05:38:58.805442Z",
     "shell.execute_reply.started": "2023-05-21T05:38:37.746310Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 12:38:39.925236: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 12:38:40.037064: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-21 12:38:44.084097: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-21 12:38:44.084169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-21 12:38:44.084176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "model_test = T5ForConditionalGeneration.from_pretrained('./fine_tuned_similar_question_generation_model_t5_small_v2')\n",
    "tokenizer_test = T5Tokenizer.from_pretrained('./fine_tuned_similar_question_generation_model_t5_small_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0aa72495-7991-4d0a-959c-71b5c40bfb28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:43:38.248055Z",
     "iopub.status.busy": "2023-05-21T05:43:38.247732Z",
     "iopub.status.idle": "2023-05-21T05:43:38.552851Z",
     "shell.execute_reply": "2023-05-21T05:43:38.552253Z",
     "shell.execute_reply.started": "2023-05-21T05:43:38.248037Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ashamed of what I did.\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm ashamed of what I did.\"\n",
    "inputs = tokenizer_test.encode(text, return_tensors=\"pt\")\n",
    "generated_ids = model_test.generate(inputs)\n",
    "generated_text = tokenizer_test.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889be609-9057-499e-b7a6-ad281b434835",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "586002e4-5b81-43fc-bef8-2fbf277978dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:43:48.387735Z",
     "iopub.status.busy": "2023-05-21T05:43:48.387416Z",
     "iopub.status.idle": "2023-05-21T05:43:48.390908Z",
     "shell.execute_reply": "2023-05-21T05:43:48.390535Z",
     "shell.execute_reply.started": "2023-05-21T05:43:48.387714Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_predictions(model, tokenizer, dataset):\n",
    "    predictions = []\n",
    "    for example in dataset:\n",
    "        input_text = example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        generated_ids = model.generate(input_ids)\n",
    "        generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "        predictions.append(generated_text)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "351f7e2a-a21d-4a54-a1bd-8dc698c1fd69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:44:33.041288Z",
     "iopub.status.busy": "2023-05-21T05:44:33.040706Z",
     "iopub.status.idle": "2023-05-21T05:44:33.045198Z",
     "shell.execute_reply": "2023-05-21T05:44:33.044750Z",
     "shell.execute_reply.started": "2023-05-21T05:44:33.041258Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_10_rows = val_dataset.select(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "69e36b59-1d5a-4a21-9018-3d00ccb864f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:44:34.796632Z",
     "iopub.status.busy": "2023-05-21T05:44:34.796222Z",
     "iopub.status.idle": "2023-05-21T05:44:34.805592Z",
     "shell.execute_reply": "2023-05-21T05:44:34.805133Z",
     "shell.execute_reply.started": "2023-05-21T05:44:34.796608Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text', 'target_text', '__index_level_0__'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_10_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9381a0ed-61b1-43f8-a1e2-a5d8002d2c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:44:38.208056Z",
     "iopub.status.busy": "2023-05-21T05:44:38.207723Z",
     "iopub.status.idle": "2023-05-21T05:44:40.648929Z",
     "shell.execute_reply": "2023-05-21T05:44:40.647787Z",
     "shell.execute_reply.started": "2023-05-21T05:44:38.208036Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinhns2/envs/python38/lib/python3.8/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "val_predictions = generate_predictions(model_test, tokenizer_test,first_10_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85aa903a-f845-4c3d-b064-ec3a7b1faf04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:44:42.616181Z",
     "iopub.status.busy": "2023-05-21T05:44:42.615829Z",
     "iopub.status.idle": "2023-05-21T05:44:42.625216Z",
     "shell.execute_reply": "2023-05-21T05:44:42.624579Z",
     "shell.execute_reply.started": "2023-05-21T05:44:42.616142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_targets = val_dataset[\"target_text\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb4e6c22-f178-487b-ad50-49950fc4bf01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:44:44.037226Z",
     "iopub.status.busy": "2023-05-21T05:44:44.036893Z",
     "iopub.status.idle": "2023-05-21T05:44:45.657823Z",
     "shell.execute_reply": "2023-05-21T05:44:45.657303Z",
     "shell.execute_reply.started": "2023-05-21T05:44:44.037198Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.5896977684739921\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "bleu_score = corpus_bleu([[target] for target in val_targets], val_predictions)\n",
    "print(f\"BLEU score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e27a4859-7e88-4f66-9a3d-e8bdd0ddfe6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T12:26:56.022408Z",
     "iopub.status.busy": "2023-05-20T12:26:56.022075Z",
     "iopub.status.idle": "2023-05-20T12:26:58.406367Z",
     "shell.execute_reply": "2023-05-20T12:26:58.405913Z",
     "shell.execute_reply.started": "2023-05-20T12:26:56.022381Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "bleu = BLEU()\n",
    "bleu.corpus_score(val_targets, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "339304f5-7f6d-4e78-8c32-7e0078d8dd6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T15:27:08.161148Z",
     "iopub.status.busy": "2023-05-14T15:27:08.160831Z",
     "iopub.status.idle": "2023-05-14T15:27:08.658159Z",
     "shell.execute_reply": "2023-05-14T15:27:08.657798Z",
     "shell.execute_reply.started": "2023-05-14T15:27:08.161131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU = 1.44 10.4/1.2/0.7/0.5 (BP = 1.000 ratio = 6.857 hyp_len = 48 ref_len = 7)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb9657d4-b96a-4194-b87d-1bf3128b3be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T12:27:00.559858Z",
     "iopub.status.busy": "2023-05-20T12:27:00.559528Z",
     "iopub.status.idle": "2023-05-20T12:27:00.622283Z",
     "shell.execute_reply": "2023-05-20T12:27:00.621928Z",
     "shell.execute_reply.started": "2023-05-20T12:27:00.559839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU = 0.76 1.6/0.9/0.6/0.4 (BP = 1.000 ratio = 6.300 hyp_len = 63 ref_len = 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = BLEU()\n",
    "bleu.corpus_score(val_targets, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d93a5dca-f3db-4dca-9d4c-121ce33b39a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T12:27:07.257880Z",
     "iopub.status.busy": "2023-05-20T12:27:07.257542Z",
     "iopub.status.idle": "2023-05-20T12:27:07.261437Z",
     "shell.execute_reply": "2023-05-20T12:27:07.261025Z",
     "shell.execute_reply.started": "2023-05-20T12:27:07.257860Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nrefs:10|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.get_signature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cbce7cd-7c29-4698-bd1b-492ea3b2a2f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T12:27:08.848174Z",
     "iopub.status.busy": "2023-05-20T12:27:08.847876Z",
     "iopub.status.idle": "2023-05-20T12:27:08.850631Z",
     "shell.execute_reply": "2023-05-20T12:27:08.850207Z",
     "shell.execute_reply.started": "2023-05-20T12:27:08.848156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chrf = CHRF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bf96928-1a7c-474a-b89a-15b1ef1a4a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T12:27:09.865411Z",
     "iopub.status.busy": "2023-05-20T12:27:09.865081Z",
     "iopub.status.idle": "2023-05-20T12:27:09.871204Z",
     "shell.execute_reply": "2023-05-20T12:27:09.870836Z",
     "shell.execute_reply.started": "2023-05-20T12:27:09.865384Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chrF2 = 16.36"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrf.corpus_score(val_targets, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f4a45a1-ed41-44aa-9f96-26d3d8d0d710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T12:27:10.985029Z",
     "iopub.status.busy": "2023-05-20T12:27:10.984716Z",
     "iopub.status.idle": "2023-05-20T12:27:10.987404Z",
     "shell.execute_reply": "2023-05-20T12:27:10.986988Z",
     "shell.execute_reply.started": "2023-05-20T12:27:10.985010Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ter = TER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1b6c67ba-33db-4f9b-acf0-e27737842a52",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-21T05:44:53.189534Z",
     "iopub.status.busy": "2023-05-21T05:44:53.189172Z",
     "iopub.status.idle": "2023-05-21T05:44:53.201974Z",
     "shell.execute_reply": "2023-05-21T05:44:53.201515Z",
     "shell.execute_reply.started": "2023-05-21T05:44:53.189511Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mter\u001b[49m\u001b[38;5;241m.\u001b[39mcorpus_score(val_targets, val_predictions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ter' is not defined"
     ]
    }
   ],
   "source": [
    "ter.corpus_score(val_targets, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ba8792c-7731-4e2e-b068-1ad0889172e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T12:27:52.833718Z",
     "iopub.status.busy": "2023-05-20T12:27:52.833306Z",
     "iopub.status.idle": "2023-05-20T12:27:52.837236Z",
     "shell.execute_reply": "2023-05-20T12:27:52.836733Z",
     "shell.execute_reply.started": "2023-05-20T12:27:52.833694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to watch a French film, right? Do you really want to see a French film?\n",
      "I haven't touched anything. I didn't touch anything.\n",
      "Let's go ask her. Let's go ask her.\n",
      "Prices are rising. Prices are rising.\n",
      "I can’t stand panties. I don't have any tolerance for cowards.\n",
      "Tom told Mary that he didn't think Alice was beautiful. Tom told Mary that he didn't think Alice was beautiful.\n",
      "I'm ashamed of what I did. I'm ashamed of my conduct.\n",
      "Tom seemed like a good kid. Tom seemed to be a decent kid.\n",
      "Tom sunbathed. Tom tanned.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,9):\n",
    "    print(val_targets[i], val_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "79755276-9f78-4d06-a003-90fb9f8429ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T05:44:59.447850Z",
     "iopub.status.busy": "2023-05-21T05:44:59.447549Z",
     "iopub.status.idle": "2023-05-21T05:44:59.450608Z",
     "shell.execute_reply": "2023-05-21T05:44:59.450228Z",
     "shell.execute_reply.started": "2023-05-21T05:44:59.447832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom is a terrible kisser. Tom is a lousy kisser.\n",
      "They took off their clothes. He took off his clothes.\n",
      "It's been funny to see how the world changes as the years go by. It would be fun to see how things change over the years.\n",
      "That stinks of cheese. It smells like cheese.\n",
      "Are they students? Are you students?\n",
      "The quick brown fox jumps over a lazy dog. Eat some more soft French buns and drink some tea.\n",
      "I know what you're up to. I know what you're planning.\n",
      "Let's hope that that happens. Let's hope that that happens.\n",
      "This morning, I was woken up by the telephone. The telephone woke me up this morning.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,9):\n",
    "    print(val_targets[i], val_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5f5ac88-1c02-4edc-ab61-3d85aec4a740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T12:41:31.039189Z",
     "iopub.status.busy": "2023-05-20T12:41:31.038839Z",
     "iopub.status.idle": "2023-05-20T12:41:31.054079Z",
     "shell.execute_reply": "2023-05-20T12:41:31.053687Z",
     "shell.execute_reply.started": "2023-05-20T12:41:31.039168Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE score: 0.02138867080699102\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4  # Define smoothing function\n",
    "rouge_scores = []\n",
    "for ref, hyp in zip(val_targets, val_predictions):\n",
    "    rouge_scores.append(sentence_bleu(ref, hyp, smoothing_function=smoothie))\n",
    "\n",
    "# Compute the average ROUGE score\n",
    "average_rouge_score = sum(rouge_scores) / len(rouge_scores)\n",
    "\n",
    "# Print the average ROUGE score\n",
    "print(\"Average ROUGE score:\", average_rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ff5b39b-4b78-4f73-af64-78da2cfe4f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-20T12:40:08.699328Z",
     "iopub.status.busy": "2023-05-20T12:40:08.698975Z",
     "iopub.status.idle": "2023-05-20T12:40:08.721178Z",
     "shell.execute_reply": "2023-05-20T12:40:08.720677Z",
     "shell.execute_reply.started": "2023-05-20T12:40:08.699303Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk.translate.ter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpus_bleu\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ter, Tercom\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Compute BLEU score\u001b[39;00m\n\u001b[1;32m      6\u001b[0m references \u001b[38;5;241m=\u001b[39m val_targets  \u001b[38;5;66;03m# List of reference paraphrases\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk.translate.ter'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.ter import ter, Tercom\n",
    "\n",
    "# Compute BLEU score\n",
    "references = val_targets  # List of reference paraphrases\n",
    "hypotheses = val_predictions  # List of generated paraphrases\n",
    "bleu_score = corpus_bleu([[ref] for ref in references], hypotheses)\n",
    "print(\"BLEU score:\", bleu_score)\n",
    "\n",
    "# Compute TER score\n",
    "ter_score = ter(hypotheses, references)\n",
    "print(\"TER score:\", ter_score)\n",
    "\n",
    "# Compute Distinct n-grams\n",
    "n = 2  # Specify the value of n for n-grams\n",
    "distinct_ngrams = set()\n",
    "for hypothesis in hypotheses:\n",
    "    tokens = hypothesis.split()\n",
    "    distinct_ngrams.update(nltk.ngrams(tokens, n))\n",
    "distinct_ngrams_count = len(distinct_ngrams)\n",
    "distinct_ngrams_score = distinct_ngrams_count / len(hypotheses)\n",
    "print(f\"Distinct {n}-grams score:\", distinct_ngrams_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce753ac6-ab54-4632-9197-89e7b17b5b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
